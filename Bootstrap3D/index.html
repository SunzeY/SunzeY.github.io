<!doctype html>
<html>
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Existing styles and scripts -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="icon" href="images/icon/icon.png">
    <script type="module" src="https://cdn.jsdelivr.net/npm/ionicons@6/dist/ionicons/ionicons.esm.js"></script>
    <script nomodule src="https://cdn.jsdelivr.net/npm/ionicons@6/dist/ionicons/ionicons.js"></script>
    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.1.1/model-viewer.min.js"></script>
    <title>Bootstrap3D</title>
    <link rel="stylesheet" href="styles.css">
  </head>
<body>
  <section class="section">
  <div class="container has-text-centered">
    <!-- paper title -->
    <p class="title is-2"> <img id="logo" width="4%" src="images/icon/boot.jpg"> Bootstrap3D: Improving 3D Content Creation with Synthetic Data</p>

    <p class="title is-5 mt-2"> 
      <a href="https://github.com/SunzeY/" target="_blank">Zeyi Sun</a><sup>1,3</sup>, 
      <a href="https://wutong16.github.io/" target="_blank">Tong Wu</a><sup>2</sup>, 
      <a href="https://panzhang0212.github.io/" target="_blank">Pan Zhang</a><sup>3</sup>,
      <a href="https://yuhangzang.github.io/" target="_blank">Yuhang Zang</a><sup>3</sup>,
      <a href="https://lightdxy.github.io/" target="_blank">Xiaoyi Dong</a><sup>3</sup>,
      <a href="http://yjxiong.me/" target="_blank">Yuanjun Xiong</a><sup>3</sup>,
      <a href="http://dahua.me/" target="_blank">Dahua Lin</a><sup>2,3</sup>
      <a href="https://myownskyw7.github.io/" target="_blank">Jiaqi Wang</a><sup>3</sup>,
    </p>
    <!-- affiliations -->
    <p class="subtitle is-5"> 
      <sup>1</sup> Shanghai Jiao Tong University,
      <sup>2</sup> The Chinese University of Hong Kong, 
      <sup>3</sup> Shanghai AI Laboratory,
      <sup>4</sup> MThreads, Inc.
    </p>
    <br>
    <!-- other links -->
    <div class="is-flex is-justify-content-center">
      <span class="icon-text mx-1">
        <a class="button is-dark" href="https://arxiv.org/abs/2404.16829" role="button" target="_blank"> <span class="icon"> <ion-icon name="document-outline"></ion-icon> </span> <span> Paper </span>  </a>
      </span>
      <span class="icon-text mx-1">
        <a class="button is-dark" href="https://github.com/SunzeY/Bootstrap3D" role="button" target="_blank"> <span class="icon"> <ion-icon name="logo-github"></ion-icon> </span> <span> Code </span> </a>
      </span>
      <span class="icon-text mx-1">
        <a class="button is-dark" href="https://huggingface.co/spaces/Zery/MV_LLaVA_demo" role="button" target="_blank"> <span class="icon"> <ion-icon name="color-wand-outline"></ion-icon> </span> <span> Demo MV-LLaVA </span> </a>
      </span>
      <span class="icon-text mx-1">
        <a class="button is-dark" target="_blank" href="https://huggingface.co/datasets/Zery/BS-Objaverse"> <span class="icon"> <ion-icon name="server-outline"></ion-icon> </span> <span> Data BS-Objaverse </span> </a>
      </span>
      <span class="icon-text mx-1">
        <a class="button is-dark" target="_blank"> <span class="icon"> <ion-icon name="color-wand-outline"></ion-icon> </span> <span> Demo MV-PixArt (Coming Soon) </span> </a>
      </span>
      <span class="icon-text mx-1">
        <a class="button is-dark" target="_blank"> <span class="icon"> <ion-icon name="images-outline"></ion-icon> </span> <span> Data BS-Synthetic3D (Coming Soon)</span> </a>
      </span>
    </div><br>
  </div>

  <!-- main container -->
  <div class="container is-max-desktop has-text-centered">
    <p class="title is-3 mt-5 has-text-centered"> Abstract </p>
    <p class="content is-size-6 has-text-left">
    <p align="left">
      Recent years have witnessed remarkable progress in multi-view diffusion models for 3D content creation. However, there remains a significant gap in image quality and prompt-following ability compared to 2D diffusion models.
      A critical bottleneck is the scarcity of high-quality 3D assets with detailed captions. To address this challenge, we propose <b>Bootstrap3D</b>,
      a novel framework that automatically generates an arbitrary quantity of multi-view images to assist in training multi-view diffusion models. 
      Specifically, we introduce a data generation pipeline that employs 
      (1) 2D and video diffusion models to generate multi-view images based on constructed text prompts, 
      and (2) our fine-tuned 3D-aware <b>MV-LLaVA</b> for filtering high-quality data and rewriting inaccurate captions. 
      Leveraging this pipeline, we have generated 1 million high-quality synthetic multi-view images with dense descriptive captions to address the shortage of high-quality 3D data. 
      Furthermore, we present a <b>Training Timestep Reschedule (TTR)</b> strategy that leverages the denoising process to learn multi-view consistency while maintaining the original 2D diffusion prior. 
      Extensive experiments demonstrate that Bootstrap3D can generate high-quality multi-view images with superior aesthetic quality, image-text alignment, and maintained view consistency.
    </p>

    <p class="title is-3 mt-5 has-text-centered">Generated gaussian given text prompts</p>
    <video muted autoplay loop> <source src="videos/gaussian.mp4" type="video/mp4"> </video>
    <p align="left">
      <b>Generated gaussian given text prompts.</b> With the help of GRM as sparse view reconstruction model, Bootstrap3D can achieve fast text-to-3D with enhanced object-text alignment and diversity compared to only training on Objaverse.
    </p>
  
    <p class="title is-3 mt-5 has-text-centered"> Generated multi-view image compared to Instant3D</p>

    <img src="images/mv_image.png" height="150%" style="margin-left: auto;margin-right: auto;display: block;"/></p><br>

    <p align="left">
      <b> Generated multi-view image compared to Instant3D.</b> Text prompts are collected from <a href="https://www.meshy.ai/" target="_blank">meshy</a> by web users.
      Through finetuning multi-view diffusion model on synthetic data with rewrited dense caption, Bootstrap3D can achieve great text control in real user cases.
    </p>

    <p class="title is-3 mt-5 has-text-centered"> Data Generation Pipeline </p>
    <img src="images/data_pipeline.png" height="150%" style="margin-left: auto;margin-right: auto;display: block;"/></p><br>

    <p align="left">
      <b> Data generation pipeline </b> consists of 1) using LLM to generate diverse text prompts 2) employing the T2I model to generate single-view images 3) synthesizing arbitrary number of multi-view images by applying the video diffusion model, 4) employing MV-LLaVA to filter and select only high-quality data, and rewrite captions to be dense and descriptive.
    </p>
    <!-- citation -->
    <div class="card mt-4">
      <header class="card-header">
        <p class="card-header-title"> Citation </p>
      </header>
      <div class="card-content is-size-5 has-text-left">
<pre><code>
  @misc{fang2024makeitreal,
    title={Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials}, 
    author={Ye Fang and Zeyi Sun and Tong Wu and Jiaqi Wang and Ziwei Liu and Gordon Wetzstein and Dahua Lin},
    year={2024},
    eprint={2404.16829},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
  }
</code></pre>
      </div>
    </div>
  </div>


  </section>
<p>
<!-- <link rel="stylesheet" href="styles.css"> -->

</div>
</p>
<div class="center-text">
  <!-- <a href="https://clustrmaps.com/site/1bzgi"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=uVAj7gydWIyD0AQWR3NPboT8GvHlNvMR9cAYCUq58m0&cl=ffffff" /></a> -->
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=a&t=tt&d=8Ib3dxikVZkaHYgFOR71ZbYgM0-UJXMMFABXL1DKDvQ&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353'></script>
</div>
</body>
</html>
